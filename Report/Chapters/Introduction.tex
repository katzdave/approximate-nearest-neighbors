\chapter{Introduction} % Main chapter title

\label{futurework} % For referencing the chapter elsewhere, use \ref{Chapter1} 

\lhead{Chapter 1. \emph{Introduction}} % This is for the header on each page - perhaps a 

Nearest neighbor search (NNS) on a vector space, described formally in Section \ref{sec:overview} has a variety of applications such as recommendation systems \citep{suchal2010full}, pattern recognition \citep{cover1967nearest}, information retrieval \citep{manning2008introduction}, and computer vision \citep{boiman2008defense}.  The most basic algorithm for NNS, a linear search as described in Section \ref{sec:linear} is guaranteed to find the exact set of nearest neighbors in linear time.  Unfortunately, for large datasets upon which many NNS queries are being performed, a linear search will take too much time.  Approximate nearest neighbor (ANN) algorithms address this concern by pre-computing and storing an index which allows computation of a result set in sublinear time as described in Section \ref{sec:ann}.  While this result set is not guaranteed to be perfectly accurate, this limitation does not prevent ANN algorithms from being used for most NNS applications.

One challenge with modeling a dataset in a vector space, is that the distance between two points, using one of the distance metrics described in Section \ref{sec:overview}, should represent the similarity of two items in the dataset.  As such, before constructing an index, each dimension needs to be normalized proportional to its importance.  This normalization to importance is equivalent to a linear transformation.  With a conventional ANN algorithm, the weights or relevance of each dimension become embedded in the index during its construction.

Thus, our goal was to design an index which can efficiently compute ANNs on queries that specify the relevance of each dimension.  To do so, we developed an system which generates a large set of indexes optimized for different dimension weights, and efficiently selects the best set of indexes to search in parallel at query time.  The particular index our system is based upon is the k-d tree.  The k-d tree is described in detail in Section \ref{sec:kdtree} and reasons for its selection in our system are described in Section \ref{sec:kdtreemotiv}.

While this type of system has many potential applications, two of the most apparent are   information retrieval and recommendation systems.  For an information retrieval system, a user can input a point, and specify the relevance of each feature to the query.  Thus, when returning similar results, our system will use a modified distance metric taking into account the specified relevance of each dimension, and will return a result set of higher quality than a conventional index in less time.  Similarly, for a recommendation system, users could specify which features are the most important to them on each query.  Doing so allows the user to have more control over the type of results a recommendation system returns, as the importance of each feature is likely not the same across all users.

To evaluate our system, we tested its performance against that of a conventional k-d tree, and a k-d optimized for the transformed vector space (which represents the theoretical best case performance of our heuristics described in Section \ref{section:splitdim} but cannot be constructed efficiently at query time).  Our result quality metric, described in Section \ref{sec:inittest}, measures the amount the average distance of points in the result set increased when compared to a linear search.  Our system was then benchmarked on multiple datasets and with different types of ANN queries.  Results are shown in Chapter \ref{results}, which describes the circumstances in which our system is most effective.