\chapter{Introduction} % Main chapter title

\label{futurework} % For referencing the chapter elsewhere, use \ref{Chapter1} 

\lhead{Chapter 1. \emph{Introduction}} % This is for the header on each page - perhaps a 

Nearest neighbor search (NNS) on a vector space, described formally in Section \ref{sec:overview} has a variety of applications including recommendation systems \citep{suchal2010full}, pattern recognition \citep{cover1967nearest}, information retrieval \citep{manning2008introduction}, image compression \citep{tong2002adaptive} and computer vision \citep{boiman2008defense}.  The most basic algorithm for NNS, a linear search as described in Section \ref{sec:linear}, is guaranteed to find the exact set of nearest neighbors in linear time.  Unfortunately, for large datasets upon which many NNS queries are being performed, a linear search will take too much time.  Approximate nearest neighbor (ANN) algorithms address this concern by pre-computing and storing an index which allows computation of a result set in sublinear time as described in Section \ref{sec:ann}.  While this result set is not guaranteed to be perfectly accurate, this limitation does not prevent ANN algorithms from being used for many NNS applications \citep{arya1996nearest}.

One challenge with modeling a dataset in a vector space is that the distance between two points, using one of the distance metrics described in Section \ref{sec:overview}, should represent the similarity of two items in the dataset.  As such, before constructing an index, each dimension needs to be normalized proportional to its importance.  This normalization to importance is equivalent to a linear transformation.  With a conventional ANN algorithm, the weights or relevance of each dimension become embedded in the index during its construction \citep{flann_pami_2014}.

Thus, our goal was to design an index which can efficiently compute ANNs on queries that specify the relevance of each dimension.  To do so, we developed a system which generates a large set of indexes optimized for different dimension weights, and efficiently selects the best set of indexes to search in parallel at query time.  The particular data structure used for our system's indexes is the k-d tree.  The k-d tree is described in detail in Section \ref{sec:kdtree} and reasons for its use by our system are described in Section \ref{sec:kdtreemotiv}.

While this type of system has many potential applications, two of the most apparent are   information retrieval and recommendation systems.  For an information retrieval system, a user would be able to input a point as a query, and specify the relevance of each feature to the query.  Our system could then use a modified distance metric taking into account the specified relevance of each dimension, leading to a result set of higher quality than a conventional index in less time.  Similarly, for a recommendation system, users could specify which features are the most important to them for each query.  Doing so would allow the user to have more control over the type of results a recommendation system returns, as the importance of each feature is likely not the same across all users.

To evaluate our system, we tested its performance against that of a conventional \mbox{k-d} tree, and a k-d optimized for the transformed vector space (which represents the theoretical best case performance of our heuristics described in Section \ref{section:splitdim}.  Our result quality metric, described in Section \ref{sec:inittest}, measures the amount that the average distance of points in the result set increased when compared to a linear search.  Our system has been benchmarked on multiple datasets and with different types of ANN queries.  Results are shown in Chapter \ref{results}, which describes the circumstances in which our system is most effective.