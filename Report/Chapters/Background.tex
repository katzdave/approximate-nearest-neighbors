\chapter{Background} % Main chapter title

\label{Background} % For referencing the chapter elsewhere, use \ref{Chapter1} 

\lhead{Chapter 2. \emph{Background}} % This is for the header on each page - perhaps a shortened title

\section{Nearest Neighbors Search}

\subsection{Overview}
\label{sec:overview}

One common way to represent a collection of objects is as a set of points in a vector space with a fixed number of dimensions.  Each object is represented as a vector, or point, in the vector space, and each dimension in the vector space represents a different feature of the object.  The distance between two points in a vector space is analagous to the relatedness of two objects.  Nearest neighbors search aims to find the closest points to a query point in a vector space.  This type of search has a variety of applications in pattern recognition \citep{cover1967nearest}, information retrieval \citep{manning2008introduction} and computer vision \citep{boiman2008defense}.

There are a variety of different types of vector spaces such as boolean valued, integer valued and mixed; however, we will focus on real-valued vector spaces.  In these spaces, the value of every dimension in a vector can be expressed as a real number.  Closeness can be defined by a variety of different distance metrics.  Some common distance metrics between two N-dimensional points, x and y, are shown in Table \ref{table:distancemet}.  In low dimensional spaces, the Euclidean distance is typically used \citep{danielsson1980euclidean}, and will be the focus of future sections.

\begin{table}
\centering
\begin{tabular}{ | l | c |}
	\hline
	Distance Type & Distance Function \\
	\hline
	Euclidean & $\sqrt{\sum\limits_{i=1}^N (x_i - y_i)^2}$ \\
	\hline
	Manhattan & $\sum\limits_{i=1}^N |x_i - y_i|$ \\
	\hline
	Chebyshev & $\max{|x_i - y_i|}$ \\
	\hline
\end{tabular}
\caption{Distance Metrics}
\label{table:distancemet}
\end{table}

\subsection{Basic Search Algorithm}
\label{sec:linear}

The most basic algorithm for a nearest neighbor search performs a linear scan across every element in a set.  Such a method computes the distance between a query point and every single point in a dataset, and returns the point with the minimum distance.  For a dataset with N points of dimensionality D, the complexity of this operation is O(ND).  For large datasets this linear time approach is not feasable, especially if many queries need to be performed.

\subsection{K-nearest Neighbors}
\label{subsec:knn}

The basic linear search algorithm can easily be extended to support a query which returns the k-nearest neighbors rather than simply the closest.  This change requires the use of a priority queue.  A priority queue guarantees amortized O(log(N)) insert and delete-max operations and constant time check-max \citep{van1976design}.  Certain implementations of a priority queue allow even faster guarantees.  For example, for binary heaps allow for average case constant time and worst case logarithmic insertion \citep{carlsson1988implicit}.  The priority metric for points will be their distance to the query point.  The first K points searched are automatically added to the priority queue.  For future points searched, if a point is closer than the furthest of the top K, the delete-max operation can be performed, and the new closer point can be added to the priority queue ensuring that K elements always remain.  If the most recently checked point is not one of the top K, then only the constant time check-max operation needs to be performed.  If the point is closer than one of the top K, the delete-max and insert operations must be performed as well.  Because both of those operations are logarithmic, the cost of updating the priority queue will at most be O(log(K)).  In practice, however, the priority queue is updated very rarely.  Assuming the points are searched in random order, the probability that a point being processed will be one of the current top K encountered is relatively low.

The k-nearest neighbors (k-NN) result is extremely useful for classification and regression on labeled datasets.  For a classification task one common way to make a hard decision on a class is to use the class that appears most frequently in the top K.  Thus, for datasets where test data is very similar to training data, this simple inference method can perform extremely well.  k-NN can also be used for regression.  Since the output is continuous in this case, the average of the results in the top K can be used.

The k-nearest neighbors has a few limitations, however \citep{beyer1999nearest}.  For one, it is an instance based learning technique.  This means that it will only perform well when instances are similar to those from training and thus does not generalize well \citep{aha1991instance}.  Another issue is the computational cost of this method.  While no training is required, each k-NN query requires linear time with respect to the size of the dataset.  Approximate nearest neighbors described in \ref{sec:ann} attempts to address this.  Another issue is that common distance metrics, such as Euclidean distance, weight each dimension equally.  Thus, in order to achieve reasonable results one must normalize all dimensions.

\section{Fixed Radius Search}

Another common search type is a fixed radius search \citep{dickerson1990fixed}.  This type of search attempts to find all points within a distance R of a query point.  The linear search algorithm can easily be adapted to this type of query.  After computing the distance between the query point and each point in the training set, if this distance is found to be less then R that point can be added to the result set.

\section{Approximate Nearest Neighbors}
\label{sec:ann}

Computation of the exact nearest neighbors via a linear search algorithm is extremely costly.  One way to improve this performance is to create an index.  The goal of an index is to increase the speed of a nearest neighbors query at the cost of additional preprocessing time and memory.  In low dimensional spaces, one common index is the k-d tree described in more detail in Section \ref{sec:kdtree}.  While the k-d tree supports average case O(log(N)) queries in low dimensional spaces, no index has beeen found which is guaranteed to return the exact set of neighbors in linear time \citep{muja_flann_2009}.

However, for many applications it is not important that the result set be perfectly accurate.  It may be advantageous to return a set which isn't guaranteed to be exact in significantly less time. For these reasons, approximate nearest neighbors are often computed instead of exact nearest neighbors.  The most common index types for approximate nearest neighbor algorithms are constructed out of trees, hash tables, or graphs \citep{flann_pami_2014}.

\subsection{Tree Based Indexes}
\label{sec:treeind}

The main concept behind a tree based index is space paritioning.  As such, these types of indexes tend to be extremely effective in low dimensionality settings, but do not scale as well to those of higher dimensionality.  Generally, at the root of these indexes, the entire search space is present \citep{yianilos1993data}.  As the tree splits, space is partitioned and only points which satisfy a split criteria will be present in each subtree.  Thus, when searching to the leaf of these trees, one can find the space partition a query point lies in, and by recursively backtracking, a process similar to the one described for k-d trees in Section \ref{sec:kdtree} can gradually expand the search radius.  K-d trees, described in detail in Section \ref{sec:kdtree}, are one of the most widely used tree based indexes.  The main advantages of k-d trees are that they are relatively fast to construct, can be easily modified, and have worst case linear space consumption \citep{yianilos1993data}.

K-means trees are another type of tree often used in practice \citep{flann_pami_2014}.  These trees are constructed with a branching factor K.  At each node, the k-means clustering algorithm is perfomed, separating remaining points into K clusters \citep{hartigan1979algorithm}.  This branching continues until less than K points remain in a node, at which point the node becomes a leaf.  To search a tree, one can move to a leaf by moving down the tree selecting the cluster with the closest mean to the query point.  As the search proceeds, each cluster's center is added to a priority queue, with its priority set as the distance from the query point.  When a leaf is reached the algorithm continues the search at the closest center in the priority queue.

K-means trees are more expensive to construct than k-d trees, as the K-means algorithm is not guaranteed to converge quickly.  Additionally, since all points are stored in the leafs and only cluster centers are stored at each node, the tree will be larger.  K-means trees, however, tend to be more effective than k-d trees when high precision is required in the result set.

Quad trees are another algorithm commonly used for nearest neighbor searches \citep{finkel1974quad}.  In a two dimensional space, each point in a quad tree splits space into 4 different quadrants, similar to how the origin separates a standard x and y axis into four regions.  The tree will be expanded this way and as such has a higher branching factor than k-d trees leading to lower depth.  Quad trees can also be expanded into octrees for 3-D space and generalized into similar higher orders for even higher dimensionality \citep{samet1988overview}.  Unfortunately, in higher dimensional spaces of dimensionality D, each point splits space into $2^D$ regions.  This often leads to many unused pointers since points will not likely lie in all of these regions.  As such, the memory cost of quad tree variants can become extremely large.

\subsection{Hash Indexes}
\label{sec:hashind}

Many variations of hash indexes exist; however, the most widely used is locality-sensitive hashing (LSH).  The goal of LSH is to use a variety of different hash functions to map similar points into the same buckets.  Rather than using cryptographic hash functions which map entities into a bucket independent of their state, the hash functions used in LSH aim to match similar points into the same bucket with a high probability, and dissimilar points into the same bucket with a low probability.  Formally, each hash function maps a D-dimensional vector v into one of R buckets \citep{datar2004locality}.  Many different types of hash functions can be used, such as projection, lattice, and quantization-based hash functions.  Different types of hash functions have been studied and evaluated extensively \citep{pauleve2010locality}.

Thus, to initialize an LSH index, one must pass every point through H hash functions, and store a key to each point within every bucket it falls into.  A larger H leads to more information in the results however requires more processing time and memory consumption.  Additionally, since all the information becomes compressed into these hash functions, these types of indexes generalize well to higher dimensional spaces.

To query an LSH index one must pass a query point through all H hash functions, and search each bucket for collisions.  The entries that most commonly collide have a smaller hamming distance in the new hash space, and will thus be treated as the most similar points.

While LSH scales extremely well to high dimensional spaces, one disadvantage is that its memory consumption tends to be much larger than that of tree based indexes in low dimensional spaces \citep{datar2004locality}.  Another key disadvantage is that quality of the search queries cannot be changed, as this is dependent on H and R.  In other words, LSH indexes have their maximum accuracy constrained during their construction, wheras tree based indexes can have variable levels of accuracy on each query dependent on the number of nodes searched.

\subsection{Graph Indexes}
\label{sec:graphind}

Graph based indexes tend to be the most expensive type to construct; however, they can support extremely fast queries.  One common type is a k-nearest neighbor graph.  In this type of graph, each node has exactly K edges, in which each node is connected to its k-nearest neighbors.  A variety of different algorithms are available for constructing these types of graphs efficiently \citep{connor2010fast}.

To perform a nearest neighbor query, a very common technique is a greedy traversal of the graph \citep{hajebi2011fast}.  Given a query point, a randomly chosen node in the graph is chosen as the startpoint.  Each neighbor is checked, and the next node traversed to is the one which is closest to the query point.  The algorithm is terminated after a fixed number of moves, where a higher number of moves will have improved results.  The K best nodes encountered are returned as the k-nearest neighbors.  Often times random resets are incooperated to ensure that different parts of the graph are searched.  Another common heuristic is to only search a randomly selected subset of the connected nodes at each node.

From experimental results, these indexes tend to perform better than LSH and k-d trees \citep{hajebi2011fast}.  However, one downside is that the offline construction of the graph is very expensive.  Additionally, there is a large amount of randomness in the search algorithm, so there tends to be a large variance in the quality of the results obtained from queries with the same point.

\section{k-d Trees}
\label{sec:kdtree}

\subsection{Overview}
The k-d tree was originally developed as ``a data structure for storage of information to be retrieved by associative searches'' \citep{bentley1975multidimensional}. k-d trees are efficient both in the speed of associative searches and in their storage requirements.  A k-d tree is a binary tree which stores points in a d-dimensional space.  Each node contains a single d-dimensional point, a split dimension, and up to two children nodes.  Each node represents a hyperplane which lies perpendicular to the split dimension and passes through the stored point.  The left subtree of a node contains all points which lie on one side of the hyperplane, while the right subtree represents all points which lie on the other side of the hyperplane.  Thus, each node partitions all nodes below it into two half-spaces.  Because only a single split dimension is used at each internal node, each splitting hyperplane is axis-aligned.  This splitting procedure continues on each subtree until each node contains only one element.  The procedure for axis selection on each split is described in section \ref{subsec:const}.

\subsection{Construction}
\label{subsec:const}

\begin{figure}
\setstretch{2}
\begin{algorithmic}
\Function{kdtree}{pointList}
	\If {pointList is empty}
		\State \Return null
	\EndIf
	\State splitDim = selectAxis()
	\State
	\State medianPoint = selectMedian(pointList, splitDim)
	\State remove medianPoint from pointList
	\State initialize empty leftList, rightList
	\ForAll{points p in pointList}
		\If {p(splitDim) $<$ medianPoint(splitDim)}
			\State leftList.Add(p)
		\ElsIf{p(splitDim) $>$ medianPoint(splitDim)}
			\State rightList.Add(p)
		\Else
			\State randomly add p to leftList or rightList with equal probability
		\EndIf
	\EndFor
	\State
	\State treenode node = new treenode()
	\State node.splitDim = splitDim
	\State node.splitPoint = medianPoint
	\State node.leftChild = kdtree(leftList)
	\State node.rightChild = kdtree(rightList)
	\State \Return node
\EndFunction
\end{algorithmic}
\caption{Pseudo code for Constructing a k-d tree}
\label{alg:createkd}
\end{figure}

The construction of a k-d tree is performed recursively with input parameters of a list of points.  Pseudo code is shown in Figure \ref{alg:createkd}.  Axis selection can be performed in multiple ways.  The classical approach is to deterministically iterate between all N dimensions.  Another approach, known as spatial median splitting, selects the the longest dimension present in the current pointList to split on \citep{zhou2008real}.  The downside of this method is that a linear traversal is required to select the split dimension.  Another popular approach is to randomly select the split dimension with an equal probability of selecting each dimension.  This approach is often applied when using multiple k-d trees; because of the additional randomness, trees are likely to be different \citep{flann_pami_2014}.  While a linear time algorithm for determining the median of an unordered set is possible \citep{megiddo1984linear}, a heuristic approach is typically used to approximate the median.  A common heuristic is to take the median of five randomly chosen elements; However many other methods can be used such as the triplet adjust method \citep{battiato2000efficient}.

At the termination of the algorithm, the root of the k-d tree is returned, and each node contains exactly one point.  The runtime of this algorithm has an average case O(Nlog(N)) running time where N is the number of points in pointList.  While the median can be approximated in constant time, partitioning pointList along that median is an O(N) operation.  Since the k-d tree is a binary tree in which each node holds one point, assuming it is relatively balanced, its height is O(log(N)).  Another key point is that points which match medianPoint along splitDim should be randomly assigned to a subtree.  This will ensure that if many points have the same value along a given dimension all of these points will be distributed close to evenly between the two subtrees allowing the tree to remain balanced.

\subsection{Nearest Neighbor Query}
\label{sec:kdquery}

A simple algorithm exists to apply the k-d tree to a nearest neighbor query.  This algorithm is guaranteed to find the single closest point to the search query.  Pseudocode for this algorithm is shown in Figure \ref{alg:querykd}.  The inputs to this algorithm are the root of the tree, the query point, and dummy current best point located at infinity in each dimension.

\begin{figure}
\setstretch{2}
\begin{algorithmic}
\Function{searchkdtree}{node, searchPoint, currBest}
	\If{node is a leaf}
		\If{dist(node.splitPoint, searchPoint) $<$ dist(currBest, searchPoint)}
			\State currBest = node.SplitPoint
		\EndIf
		\State \Return
	\EndIf
	\State
	\State dim = node.splitDim
	\State Boolean searchDir = searchPoint[dim] $<$ node.splitPoint[dim]
	\State searchFirst = searchDir ? node.left : node.right
	\State searchSecond = searchDir ? node.right : node.left
	\State searchkdtree(searchFirst, searchPoint, currBest)
	\State
	\If{dist(node.splitPoint, searchPoint) $<$ dist(currBest, searchPoint)}
		\State currBest = node.SplitPoint
	\EndIf
	\If{HyperPlaneCheck(searchPoint, currBest, searchSecond)}
		\State searchkdtree(searchSecond, searchPoint, currBest)
	\EndIf

\EndFunction
\end{algorithmic}
\caption{Pseudo code to apply a nearest neighbor search using a k-d tree}
\label{alg:querykd}
\end{figure}

The first part of the algorithm recursively steps down the tree until a leaf is reached.  At each node, a comparison on the split dimension is performed to determine which side of the splitting hyperplane the search point lies so that the search can continue in the proper half space.  When a leaf is reached, the point stored in the leafnode is set as the current closest point.  The algorithm then recursively walks back up the tree, and at each node computes the difference between the current node's point and the searchpoint.  If this distance is smaller than that of the current best, the current node point becomes the current best.

The algorithm then determines whether a closer point than the current best could potentially exist in the second unsearched subtree.  Because all hyperplanes are axis aligned, this computation is very simple.  The closest possible point in the halfspace represented by the second subtree could potentially lie within a distance of $\epsilon$ from the hyperplane, where $\epsilon$ is very small.  The lower bound of the distance of the closest point, occuring when $\epsilon$ approaches zero, is the absolute value of the difference between the search point and split point along the current split dimension.  If this distance is larger than the current best point's distance, then the algorithm does not need to check the second subtree, as there is no possible closer point in that halfspace.  If this distance is smaller however, then the algorithm will search down the second subtree following the exact same procedure as before, treating the second child as the root.  Because of this comparison however, the worst case running time of this algorithm is O(N), as if all comparisons fail, then the entirity of the tree will be searched.  As the dimensionality of the tree becomes larger, this check is more likely to fail, and k-d trees diminish in effectiveness.

This algorithm can be extended to support KNN with the same priority queue procedure described in Section \ref{subsec:knn}.  The check as to whether a closer point could potentially exist in the second subtree will then be performed against the furthest point in the top K.  This algorithm can also be extended to perform a radius bounded search.  Rather than checking the distance to the hyperplane compared to the furthest point in the top K one can check against a fixed radius R.  Thus, this algorithm can also efficiently find all points within R of the query point.

\subsection{Approximate Nearest Neighbors Query}

The k-d tree nearest neighbor search algorithm can be extended into an approximate nearest neighbors search.  To do so, a limit on the number of points to search must be applied.  The algorithm will follow the exact same steps as shown in Figure \ref{alg:querykd}; however, when the search limit is reached the algorithm terminates.  This means that not every possible node that might contain a closer point would necessarily be searched.  However, the nodes that do get searched are more likely to contain the closer points.  In other words, the algorithm will examine the regions of space which are closest to the search query first before expanding outward.

\subsection{Modification}
\label{sec:kdmod}

One key advantage of k-d trees is that they are very easy to modify.  Inserting a node requires a traversal to a leaf node following the same procedure as described in the first part of Figure \ref{alg:querykd}.  This search takes approximate O(log(N)) time if the tree is balanced.  Once a leaf node is reached, a single comparison along the split dimension needs to be performed in order determine whether the new node should be added as the left or right child.  If random points are inserted on a balanced tree, the tree will remain relatively balanced as there will be an equal probability of placing points below each leaf.  Heuristics can be used to help ensure that a tree remains balanced in a dyanmic environment \citep{hunt2006fast}.

Deletion on k-d trees can also be performed with relative ease.  Again a downward traversal is performed until the target node is encountered. If the target node is a leaf, it can be removed trivially by removing the connection from its parent.  If the target node is not a leaf, lazy deletion can be performed.  The node will not be removed and will still serve to partition space; however it will not store any actual split point against which comparisons can be made.  If many nodes are lazy deleted, performance will slowly degrade, and reconstruction of the tree may be advisable.

\section{Randomized k-d Tree Forests}
\label{sec:randomforest}

With one k-d tree, one runs the risk of potentially very bad queries occuring when one of the earlier splitting hyperplanes lies close to the point of interest.  By having a forest of multiple k-d trees made from randomized split dimensions and splitpoints, the effects of this can be minimized.  \citep{silpa2008optimised} has shown that k-d tree forests can lead to inproved performance.  It is important to note that when searching multiple trees, the same heap must be used, and as such this object needs to properly be mutex locked if the search is to occur in parallel.  As better results are found in each tree, more hyperplane checks will pass and less time will be wasted searching parts of the tree where better results do not lie.  When searching trees sequentially, rather than in parallel, the performance was significantly weaker \citep{silpa2008optimised}.

Another approach to improve performance is the rotation of dimensions along different trees \citep{silpa2008optimised}.  This allows the splitting hyperplanes to no longer be constrained to axis alignment, and can introduce a greater amount of variety in the forest.  Based on experimental results, the optimal amount of trees was generally less than 20, though the number varied greatly between different datasets \citep{muja_flann_2009}.