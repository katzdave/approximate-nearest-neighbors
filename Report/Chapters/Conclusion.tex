\chapter{Conclusion} % Main chapter title

\label{conclusion} % For referencing the chapter elsewhere, use \ref{Chapter1} 

\lhead{Chapter 5. \emph{Conclusion}} % This is for the header on each page - perhaps a 

\section{Future Work}

\subsection{Heuristic Tuning}

While it was demonstrated that the heuristics selected provided significant performance improvements over a standard k-d tree, they are likely not the optimal heuristics for every dataset.  One way to improve performance would be tune split heuristics specific to the distribution of a dataset.  For example, for a dataset with a uniform distribution across all dimensions, spatial median splitting will perform very well.  However, for a dataset which consists of distinct Gaussian distributed clusters, an alternative heuristic could lead to improvements.  In general, more complicated heuristics such as surface area heuristics have demonstrated improved performance with a higher offline computational cost \citep{hunt2006fast}.  If these heuristics were adapted to the constraint of DRVs, further improvements in the quality of the result set given a fixed number of searches could likely be made.  Tuning this heuristic would effect the theoretical best case scenario when the seed and query DRVs match.

Another route for potential improvement is to improve the tree selection heuristic.  In the current implementation, a distance metric is used to provide a scalar value which represents the quality of a tree.  However, another potential approach would be to keep track of how well a seed DRV matches a query DRV in all dimensions.  This would mean using a vector based district metric rather than a scalar.  In doing so, rather than simply using the trees which best match the DRV overall, a set of trees could be extracted which attempt to make sure each dimension is represented with proportional relevance across all of the trees. In doing so, one could avoid selecting a set of trees in which the same dimensions are over or under represented in each.

\subsection{Real World Considerations}
\label{sec:realworld}

There are a variety of aspects which must be considered when attempting a real world of this algorithm.  The first is the large memory cost.  As shown in Section \ref{results}, a larger amount of trees results in improved performance, as the overall quality of the top trees will be higher.  However, there is a diminishing return to this effect, so memory consumption should be balanced and tuned.  One proposed method of tuning is to develop a cost metric which considers both memory, and the quality of results.  Using a small initial set of requested or randomly generated queries, one can use this data to minimize this cost metric, and select the optimal memory consumption.  Other algorithmic parameters can be tuned in a similar manner.  By examining the performance relative to a linear search on a subset of queries, one can attempt these queries with different parameters to see if performance can be improved.  \citep{muja_flann_2009} suggests some methods for automatic parameter selection such as putting weights on different cost aspects of the algorithm.

If the dataset is large, a single server implementation is likely not possible as the memory consumption of this algorithm can be hundreds of times larger than the original dataset.  To counteract this, we propose a distributed approach with N trees split across M compute nodes, with some duplication factor D.  The duplication factor represents, the number of different nodes in which a single tree exists in memory on.  A larger duplication factor will further increase memory cost by D times, but will allow the system to be more robust in the event of a node's failure.  \citep{nitzberg1991distributed} discusses many types of potential architectures for a system with distrubed memory, and considerations of each.

In our suggested distributed implementation, each node will store as many trees as it can fit into memory, and will also store the seed DRVs used to construct each tree, and the nodes which hold each of these trees.  A load balancer should be applied to determine which nodes requests come through to based on how busy they currently are \citep{cybenko1989dynamic}.  When a request arrives at a node, that node can compute the best set of trees to search, and the least busy nodes which holds those trees.  This selection process can be augmented by adding an additional cost to trees whose nodes are currently busy.  In doing so, the search can be performed with slightly lower quality to avoid waiting on nodes to free up.  This system could also be implementated with no duplication (D = 1).  While this would result in less memory consumption, in the event of a node shutting down all of its trees would need to be reconstructed in others.  During this time, those trees would be unavailable, which would temporarily hinder performance on queries.

After the top trees and their nodes are selected for a query, a distributed priority and hashtable must be used to perform a parallel search on them.  Considerations for these data structures are discussed in \citep{kaashoek2003koorde} and \citep{rogers1995supporting}.  It is also important to ensure that the searches happen in relative parallel for best results.  For example, if one node searches its allocated amount before another, results will not be as accurate as if the searches are interleaved between the two nodes.  In our testbed, searches were perfectly interleaved relative to the quality of each tree.  Thus, real world results will likely not reach these benchmarks.

Another important consideration with this distributed system is future performance tuning.  In our proposed construction of the data structure, no information was available about the frequencies of different DRVs in each query, and as such a uniform distribution of seed DRVs was used.  However, by storing all requested DRVs, one can learn which type of queries are most popular.  The system can then eliminate the trees which are least used, and replace them with trees which better match the popular queries.  This dynamic adjustment will allow the system's performance to improve over time as more data about the types of searches performed is gathered.

Another potential concern is a dataset large enough such that it cannot fit in memory on a single node.  In this case, subtrees must be included on separate nodes containing parts of the dataset.  Performance concerns on a distributed k-d trees are discussed in \citep{aly2011distributed}.  Another potential solution is to perform the ANN queries on disk.  This is far from ideal as disk read and write times are significantly larger than those of memory \citep{karedla1994caching}.  The primary cost encountered would become page reads and writes, which k-d trees are not optimized for.  In this case, a similar system to ours could be constructed using R-trees, a similar tree to k-d trees which is optimized to minimize disk costs \citep{guttman1984r}.

Lastly, one must consider performance on a changing dataset.  In particular, datasets tend to grow over time and as such it is important to support efficient insertion of new data.  To insert a new item, it must be added to every single k-d tree.  As mentioned in section \ref{sec:kdmod}, the insertion of a new element into a k-d tree is O(Log(N)).  However, over time trees are likely to become unbalanced, and the quality of results will begin to degrade.  One approach of avoiding the issue would be to constantly rebuild trees which have a large number of inserted nodes.  This could be performed one tree at a time, so the system could remain live while updating its trees in the background.  However, when this cost is introduced the offline construction cost of trees becomes more imporant, as trees will be constructed during system use.  Thus, for a constantly growing data structure there may be some value in the random splitting heuristic which while having weaker query performance could be constructed faster due to not requiring a linear seek on each dimension.

