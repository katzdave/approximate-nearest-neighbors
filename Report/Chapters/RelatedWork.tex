\chapter{Related Work} % Main chapter title

\label{Related Work} % For referencing the chapter elsewhere, use \ref{Chapter1} 

\lhead{Chapter 2. \emph{Related Work}} % This is for the header on each page - perhaps a shortened title

\section{Approximate Nearest Neighbor Frameworks}

Many ANN frameworks exist for fast computation.  Since no exact nearest neighbors algorithms are faster than linear time in high dimensional spaces, a variety of different ANN algorithms must be applied.  The Fast Library for Approximate Nearest Neighbors (FLANN) makes use of many of these algorithms including k-d trees, k-means trees, and locality sensitive hashing \citep{muja_flann_2009}.  However, each of these types of indexes have different properties, and some may be suited to some datasets more than others.  Thus, one difficult task this framework can perform is automatic selection of the index type and parameters.  This is done by estimating the overall cost of an index in terms of memory consumption, index generation time, and query speed to achieve a given accuracy.  The user of the system can also put weights on each of these costs to raise the relative importance of one ore more of these factors.  By benchmarking on a small subset of a dataset, the framework can effectively select the most efficient index type and apply the simplex optimization algorithm \citep{nelder1965simplex} to optimize its parameters.

Also of importance with FLANN and other frameworks is that they are optimized for real world performance.  This means that that benchmarks are taken in terms of real time, and memory consumption is minimized.  For example, when possible bit array keys are used rather than pointers in order to save as much memory as possible.