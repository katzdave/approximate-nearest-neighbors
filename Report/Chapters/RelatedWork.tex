\chapter{Related Work} % Main chapter title

\label{Related Work} % For referencing the chapter elsewhere, use \ref{Chapter1} 

\lhead{Chapter 3. \emph{Related Work}} % This is for the header on each page - perhaps a shortened title

\section{Approximate Nearest Neighbor Frameworks}

Many ANN frameworks exist for fast computation.  Since no exact nearest neighbors algorithms are faster than linear time in high dimensional spaces, a variety of more efficient ANN algorithms exist.  The Fast Library for Approximate Nearest Neighbors (FLANN) makes use of many of these algorithms including k-d trees, k-means trees, and locality sensitive hashing \citep{muja_flann_2009}.  However, each of these types of indexes have different properties, and some may be better suited to certain datasets.  Thus, one difficult task this framework can perform is automatic selection of the index type and parameters.  This is done by estimating the overall cost of an index in terms of memory consumption, index generation time, and query speed to achieve a given accuracy.  The user of the system can also set weights to raise the relative importance of one or more of these factors.  By benchmarking on a small subset of a dataset, the framework can effectively select the most efficient index type and apply the simplex optimization algorithm to optimize its parameters \citep{nelder1965simplex}.

Also of importance with FLANN and other frameworks is that they are optimized for real world performance.  This means that that benchmarks are taken in terms of real time in controlled test environment \citep{muja_flann_2009}.  Additionally, physical memory consumption is another concern of these frameworks.  To minimize physical memory consumption a variety of optimizations exist; for example, bit array keys are used rather than pointers.